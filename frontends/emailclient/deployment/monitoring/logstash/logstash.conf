# Logstash Configuration - Phase 8 Email Client Log Aggregation
# Processes logs from all services and forwards to Elasticsearch
# Last Updated: 2026-01-24

input {
  # ============================================================================
  # Docker Logs via TCP
  # ============================================================================
  tcp {
    port => 5000
    codec => "json"
    type => "docker-logs"
    tags => ["docker"]
  }

  # ============================================================================
  # Syslog Input from Postfix/Dovecot
  # ============================================================================
  syslog {
    port => 514
    type => "syslog"
    tags => ["email-server"]
  }

  # ============================================================================
  # Application Logs via HTTP
  # ============================================================================
  http {
    port => 8080
    codec => "json"
    type => "app-logs"
    tags => ["application"]
  }

  # ============================================================================
  # File Inputs - Email Service Logs
  # ============================================================================
  file {
    path => "/var/log/emailclient/email-service/app.log"
    codec => "multiline"
    {
      pattern => "^%{TIMESTAMP_ISO8601}"
      negate => false
      what => "previous"
    }
    type => "email-service"
    tags => ["email-service", "app-logs"]
  }

  # ============================================================================
  # File Inputs - Celery Worker Logs
  # ============================================================================
  file {
    path => "/var/log/emailclient/celery/worker.log"
    codec => "multiline"
    {
      pattern => "^\[%{TIMESTAMP_ISO8601}"
      negate => false
      what => "previous"
    }
    type => "celery-worker"
    tags => ["celery", "app-logs"]
  }

  # ============================================================================
  # File Inputs - PostgreSQL Logs
  # ============================================================================
  file {
    path => "/var/log/postgresql/*.log"
    codec => "multiline"
    {
      pattern => "^%{TIMESTAMP_ISO8601}"
      negate => false
      what => "previous"
    }
    type => "postgres"
    tags => ["database", "system-logs"]
  }

  # ============================================================================
  # File Inputs - Postfix Logs
  # ============================================================================
  file {
    path => "/var/log/postfix/mail.log"
    type => "postfix"
    tags => ["postfix", "email-server"]
  }

  # ============================================================================
  # File Inputs - Dovecot Logs
  # ============================================================================
  file {
    path => "/var/log/dovecot/*.log"
    type => "dovecot"
    tags => ["dovecot", "email-server"]
  }
}

filter {
  # ============================================================================
  # Email Service Log Parsing
  # ============================================================================
  if [type] == "email-service" {
    grok {
      match => {
        "message" => "^\[%{TIMESTAMP_ISO8601:timestamp}\] %{LOGLEVEL:level} - %{DATA:component}: %{GREEDYDATA:msg}"
      }
      remove_field => ["message"]
    }

    mutate {
      add_field => { "[@metadata][index_name]" => "logs-email-service" }
    }

    # Extract HTTP request details
    if [msg] =~ /^(GET|POST|PUT|DELETE|PATCH)/ {
      grok {
        match => {
          "msg" => "^%{WORD:http_method} %{DATA:http_path} %{WORD:http_protocol} %{NUMBER:http_status} %{NUMBER:response_time}ms"
        }
      }
    }
  }

  # ============================================================================
  # Celery Worker Log Parsing
  # ============================================================================
  if [type] == "celery-worker" {
    grok {
      match => {
        "message" => "^\[%{TIMESTAMP_ISO8601:timestamp}\] %{LOGLEVEL:level} %{DATA:task_name}: %{GREEDYDATA:task_msg}"
      }
      remove_field => ["message"]
    }

    mutate {
      add_field => { "[@metadata][index_name]" => "logs-celery-worker" }
    }

    # Parse task execution times
    if [task_msg] =~ /execution time:/ {
      grok {
        match => {
          "task_msg" => "execution time: %{NUMBER:execution_time:float}s"
        }
      }
    }
  }

  # ============================================================================
  # PostgreSQL Log Parsing
  # ============================================================================
  if [type] == "postgres" {
    grok {
      match => {
        "message" => "^\[%{TIMESTAMP_ISO8601:timestamp}\] %{LOGLEVEL:level}: %{GREEDYDATA:query}"
      }
      remove_field => ["message"]
    }

    mutate {
      add_field => { "[@metadata][index_name]" => "logs-postgresql" }
    }

    # Extract slow query info
    if [query] =~ /duration:/ {
      grok {
        match => {
          "query" => "duration: %{NUMBER:query_duration:float} ms"
        }
      }
    }
  }

  # ============================================================================
  # Postfix Log Parsing
  # ============================================================================
  if [type] == "postfix" {
    grok {
      match => {
        "message" => "%{SYSLOGLINE}"
      }
    }

    mutate {
      add_field => { "[@metadata][index_name]" => "logs-postfix" }
    }

    # Parse SMTP status
    if [message] =~ /status=bounced|status=deferred|status=sent|status=held/ {
      grok {
        match => {
          "message" => "status=(?<smtp_status>\S+)"
        }
      }
    }
  }

  # ============================================================================
  # Dovecot Log Parsing
  # ============================================================================
  if [type] == "dovecot" {
    grok {
      match => {
        "message" => "^[A-Za-z]{3}\s+%{INT:day}\s%{TIME:time}\s%{HOSTNAME:host}\s%{GREEDYDATA:msg}"
      }
    }

    mutate {
      add_field => { "[@metadata][index_name]" => "logs-dovecot" }
    }

    # Parse authentication events
    if [msg] =~ /auth.*success|auth.*failed/ {
      grok {
        match => {
          "msg" => "auth %{WORD:auth_method}.*(?<auth_status>success|failed)"
        }
      }
    }
  }

  # ============================================================================
  # Common Parsing
  # ============================================================================

  # Add timestamp if not already present
  if ![timestamp] {
    mutate {
      add_field => { "[@metadata][timestamp]" => "%{@timestamp}" }
    }
  }

  # Normalize log levels
  if [level] {
    translate {
      field => "level"
      destination => "level_normalized"
      dictionary => {
        "DEBUG" => "debug"
        "INFO" => "info"
        "WARNING" => "warning"
        "ERROR" => "error"
        "CRITICAL" => "critical"
      }
      fallback => "%{level}"
    }

    mutate {
      replace => { "level" => "%{level_normalized}" }
      remove_field => ["level_normalized"]
    }
  }

  # Add metadata tags
  mutate {
    add_field => {
      "service" => "%{type}"
      "cluster" => "emailclient-phase8"
      "environment" => "production"
      "tenant_id" => "default"
    }
  }

  # ============================================================================
  # Metric Extraction for Alerts
  # ============================================================================
  if [response_time] or [execution_time] or [query_duration] {
    ruby {
      code => '
        event.set("response_time_numeric", event.get("response_time_numeric") || event.get("response_time")&.to_f)
        event.set("execution_time_numeric", event.get("execution_time_numeric") || event.get("execution_time")&.to_f)
        event.set("query_duration_numeric", event.get("query_duration_numeric") || event.get("query_duration")&.to_f)
      '
    }
  }

  # Set default index name if not already set
  if ![@metadata][index_name] {
    mutate {
      add_field => { "[@metadata][index_name]" => "logs-%{type}" }
    }
  }
}

output {
  # ============================================================================
  # Elasticsearch Output
  # ============================================================================
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_name]}-%{+YYYY.MM.dd}"
    user => "elastic"
    password => "${ELASTICSEARCH_PASSWORD:changeme}"
    ssl => false
    manage_template => true
    template => "/etc/logstash/templates/email-service-template.json"
    template_name => "email-service-logs"
    template_overwrite => false
  }

  # ============================================================================
  # Console Output for Debugging (Development Only)
  # ============================================================================
  # stdout {
  #   codec => rubydebug
  # }

  # ============================================================================
  # File Output for Backup
  # ============================================================================
  file {
    path => "/var/log/logstash/processed/%{[@metadata][index_name]}-%{+YYYY-MM-dd}.log"
    create_if_deleted => true
  }
}
